{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Module 05: Lab 02\"\n",
    "subtitle: \"Regression Modeling on Employment Data\"\n",
    "author: Yanyang He\n",
    "number-sections: true\n",
    "date: \"2025-04-14\"\n",
    "format:\n",
    "  html:\n",
    "    theme: cerulean\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "date-modified: today\n",
    "date-format: long\n",
    "execute: \n",
    "  echo: false\n",
    "  eval: false\n",
    "  freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives {.unnumbered}\n",
    "\n",
    "1. Use **PySpark** to process the Lightcast dataset.\n",
    "2. Engineer features from structured columns for salary prediction.\n",
    "3. Train **Linear Regression model**.\n",
    "4. Evaluate models using **RMSE** and **R²**.\n",
    "5. Visualize predictions using diagnostic plots.\n",
    "6. Push work to GitHub and submit the repository link.\n",
    "\n",
    "# Setup {.unnumbered}\n",
    "\n",
    "The instruction below provides you with general keywords for columns used in the lightcast file. See the data schema generated after the load dataset code above to use proper column name. For visualizations, tables, or summaries, please **customize colors, fonts, and styles** as appropriate to avoid a **2.5-point deduction**. Also, **provide a two-sentence explanation** describing key insights drawn from each section's code and outputs. \n",
    "\n",
    "1. Follow the steps below as necessary, use your best judgement in importing/installing/creating/saving files as needed.\n",
    "2. Create a new Jupyter Notebook in your `ad688-sp25-lab08` directory named `lab08_yourname.ipynb`, if the file exists make sure to change the name.\n",
    "3. Use your **EC2 instance** for this lab.\n",
    "4. Ensure the `lightcast_data.csv` file is available on the EC2 instance. if not then **Download the dataset**\n",
    "5. **Add the dataset to `.gitignore`** to avoid pushing large files to GitHub. Open your `.gitignore` file and add:\n",
    "6. Make sure to create a virtual environment and install the required Python libraries if needed, don't forget to activate it:\n",
    "7. Install the required Python libraries if needed, you can also use the given requirement file to install the packages to the virtual environment:\n",
    "\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "gdown https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\n",
    "echo \"lightcast_job_postings.csv\" >> .gitignore\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load the Dataset\n",
    "1. **Load the Raw Dataset**:\n",
    "   - Use Pyspark to the `lightcast_data.csv` file into a DataFrame:\n",
    "   - You can reuse the previous code. \n",
    "   - [Copying code from your friend constitutes plagiarism. DO NOT DO THIS]{.uured-bold}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 17:09:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#| eval: true\n",
    "#| echo: true\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 20)\n",
    "\n",
    "# Load Data\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Feature Engineering is a crucial step in preparing your data for machine learning. In this lab, we will focus on the following tasks:\n",
    "\n",
    "1. **Drop rows with missing values** in the target variable and key features.\n",
    "2. By now you are already familiar with the code and the data. Based on your understanding please choose any 3 (my code output has 10) variables as:\n",
    "   1. two continuous variables (use your best judgment!)\n",
    "   2. one categorical.\n",
    "   3. Your dependent variable (y) is `SALARY`.\n",
    "3. **Convert categorical variables** into numerical representations using **StringIndexer** and **OneHotEncoder**.\n",
    "4. **Assemble features** into a single vector using **VectorAssembler**.\n",
    "5. **Split the data** into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+\n",
      "|SALARY|features                   |\n",
      "+------+---------------------------+\n",
      "|192800|(7,[0,1,2],[55.0,6.0,1.0]) |\n",
      "|125900|(7,[0,1,5],[18.0,12.0,1.0])|\n",
      "|118560|(7,[0,1,3],[20.0,5.0,1.0]) |\n",
      "|192800|(7,[0,1,2],[55.0,6.0,1.0]) |\n",
      "|116500|(7,[0,1,5],[16.0,12.0,1.0])|\n",
      "+------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "#| echo: true\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "df = df.dropna(subset=[\n",
    "    \"SALARY\",\n",
    "    \"REMOTE_TYPE_NAME\",\n",
    "    \"DURATION\",\n",
    "    \"MIN_YEARS_EXPERIENCE\",\n",
    "])\n",
    "# from previous labs, we know that average salary of remote/on site/hybrid are quite different\n",
    "categorical_cols = [\"MIN_EDULEVELS_NAME\"]\n",
    "# we only have two numerical variables: Duration and YOE\n",
    "continuous_cols = [\"DURATION\", \"MIN_YEARS_EXPERIENCE\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid='skip') for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_vec\") for col in categorical_cols]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=continuous_cols + [f\"{col}_vec\" for col in categorical_cols],\n",
    "    outputCol=\"features\",\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "data = pipeline.fit(df).transform(df).select(\"SALARY\", \"features\")\n",
    "data.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "\n",
    "- Perform a **random split** of the data into training and testing sets.\n",
    "- Set a random seed for reproducibility.\n",
    "- You can choose a number for splitting to your liking, justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "14416"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10217, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4199, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# After dropping NA, we have only 14k samples.\n",
    "# The dataset is small, so I chooe to split by 70/30.\n",
    "# The seed is set to 42 for reproducibility\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "print((train_data.count(), len(train_data.columns)))\n",
    "print((test_data.count(), len(test_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "- Train a **Linear Regression** model using the training data. [You will run in to an important issue here. Please make an effort in figuring it by yourself. This is one of the most asked interview questions in CapitalOne's management recruiting program.]{.uured-bold}\n",
    "- Evaluate the model on the test data.\n",
    "- Print the coefficients, intercept, R², RMSE, and MAE.\n",
    "- Use the `summary` object to extract the coefficients and their standard errors, t-values, and p-values.\n",
    "- Create a DataFrame to display the coefficients, standard errors, t-values, p-values, and confidence intervals.\n",
    "- Interpret the coefficients and their significance and explain the model performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 17:35:38 WARN Instrumentation: [bac1477f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# train a linear regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"SALARY\", predictionCol=\"prediction\")\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.375663</td>\n",
       "      <td>33844.806259</td>\n",
       "      <td>26245.314927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         r2          rmse           mae\n",
       "0  0.375663  33844.806259  26245.314927"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model summary\n",
    "summary = lr_model.summary\n",
    "pd.DataFrame(dict(\n",
    "    r2 = [summary.r2],\n",
    "    rmse = [summary.rootMeanSquaredError],\n",
    "    mae = [summary.meanAbsoluteError],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretaion of model summaries:\n",
    "\n",
    "- R-square 0.376: only 37% variation in salries can be explained by the model\n",
    "- RMSE and MAE: very large, the model cannot give precise predictions\n",
    "\n",
    "We have only take 3 variables as input, many other variables may play have significant influence on salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33134.004218</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>25758.992898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse        r2           mae\n",
       "0  33134.004218  0.369048  25758.992898"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on test data\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# supported metrics: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html\n",
    "test_eval = dict()\n",
    "for metric in [\"rmse\", \"r2\", \"mae\"]:\n",
    "    evaluator = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=metric)\n",
    "    value = evaluator.evaluate(predictions)\n",
    "    test_eval[metric] = [value]\n",
    "\n",
    "pd.DataFrame.from_dict(test_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreations of results on test set:\n",
    "\n",
    "- RMSE/MAE/R-Square are similar to the result on training set\n",
    "- our model can genearlize to unseen data\n",
    "- it has a consistent performance on train/test set\n",
    "\n",
    "However, as we said before, the model cannot give a precise prediction and can only explain for 37% of the variation in the salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept and Coefficients</th>\n",
       "      <th>Std Error</th>\n",
       "      <th>t-stat</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98812.681381</td>\n",
       "      <td>23.372127</td>\n",
       "      <td>-0.313938</td>\n",
       "      <td>7.535746e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.337397</td>\n",
       "      <td>106.549389</td>\n",
       "      <td>70.225843</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7482.520653</td>\n",
       "      <td>8222.987333</td>\n",
       "      <td>-2.011999</td>\n",
       "      <td>4.424626e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-16544.642263</td>\n",
       "      <td>8257.008684</td>\n",
       "      <td>-1.785402</td>\n",
       "      <td>7.422586e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14742.077469</td>\n",
       "      <td>8305.820642</td>\n",
       "      <td>-5.765106</td>\n",
       "      <td>8.397403e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-47883.936060</td>\n",
       "      <td>8337.317900</td>\n",
       "      <td>-5.931231</td>\n",
       "      <td>3.104303e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-49450.554328</td>\n",
       "      <td>8494.582812</td>\n",
       "      <td>3.285598</td>\n",
       "      <td>1.021094e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27909.782503</td>\n",
       "      <td>8239.300243</td>\n",
       "      <td>11.992849</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept and Coefficients    Std Error     t-stat       P-Value\n",
       "0                98812.681381    23.372127  -0.313938  7.535746e-01\n",
       "1                   -7.337397   106.549389  70.225843  0.000000e+00\n",
       "2                 7482.520653  8222.987333  -2.011999  4.424626e-02\n",
       "3               -16544.642263  8257.008684  -1.785402  7.422586e-02\n",
       "4               -14742.077469  8305.820642  -5.765106  8.397403e-09\n",
       "5               -47883.936060  8337.317900  -5.931231  3.104303e-09\n",
       "6               -49450.554328  8494.582812   3.285598  1.021094e-03\n",
       "7                27909.782503  8239.300243  11.992849  0.000000e+00"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show intercept and coefficients\n",
    "pd.DataFrame({\n",
    "    \"Intercept and Coefficients\": [lr_model.intercept] + list(lr_model.coefficients),\n",
    "    \"Std Error\": summary.coefficientStandardErrors,\n",
    "    \"t-stat\": summary.tValues,\n",
    "    \"P-Value\": summary.pValues\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretations:\n",
    "\n",
    "- Intercept: This is the base salary level when no information is provided. Small std-err and very low p-value. The intercept is good enough.\n",
    "- coefficient for duration: Negatively correlated to the salary. This means jobs that high paying jobs are quickly filled on the job market. However, this factor is not very significant, one may day open reduces the salary prediction by only 7.3$\n",
    "- coefficient for years of experience: positively related to the salary. This means the more experience one have, the higher salary they get.\n",
    "- coefficient for remote: On-site jobs are likely to have higher salaries (feature 7).\n",
    "- For REMOTE type features: the coefficient absolute values are very large, meaning that remote/hybrid/onsite greatly influence the salary.\n",
    "- The standard error of one-hot vector features are very large, meaning that the coefficient are mot reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3268840423.py, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 49\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"Length of features:\", len(features))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Use the summary object to extract the coefficients and their standard errors, t-values, and p-values.\n",
    "# Create a DataFrame to display the coefficients, standard errors, t-values, p-values, and confidence intervals.\n",
    "\n",
    "\n",
    "\n",
    "coef_df.index = [\"Intercept\"] + [f\"Feature {i+1}\" for i in range(len(lr_model.coefficients))]\n",
    "\n",
    "\n",
    "coeff_data = zip(\n",
    "    [\"Intercept\"] + feature_names,\n",
    "    [lr_model.intercept] + list(lr_model.coefficients),\n",
    "    summary.coefficientStandardErrors,\n",
    "    summary.tValues,\n",
    "    summary.pValues\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Estimate\": [lr_model.intercept] + list(lr_model.coefficients),\n",
    "    \"Std Error\": lr_summary.coefficientStandardErrors,\n",
    "    \"t-stat\": lr_summary.tValues,\n",
    "    \"P-Value\": lr_summary.pValues\n",
    "})\n",
    "\n",
    "coef_df.index = [\"Intercept\"] + [f\"Feature {i+1}\" for i in range(len(lr_model.coefficients))]\n",
    "\n",
    "coef_df.head()\n",
    "\n",
    "\n",
    "# Summary stats\n",
    "print(\"\\n--- Regression Summary ---\")\n",
    "print(\"Coefficient Standard Errors:\", [f\"{val:.4f}\" for val in summary.coefficientStandardErrors])\n",
    "print(\"T Values:\", [f\"{val:.4f}\" for val in summary.tValues])\n",
    "print(\"P Values:\", [f\"{val:.4f}\" for val in summary.pValues])\n",
    "\n",
    "# print(f\"\\nDispersion: {summary.dispersion:.4f}\")\n",
    "# print(f\"Null Deviance: {summary.nullDeviance:.4f}\")\n",
    "# print(f\"Residual DF Null: {summary.residualDegreeOfFreedomNull}\")\n",
    "# print(f\"Deviance: {summary.deviance:.4f}\")\n",
    "# print(f\"Residual DF: {summary.residualDegreeOfFreedom}\")\n",
    "# print(f\"AIC: {summary.aic:.4f}\")\n",
    "\n",
    "# 1. Pull feature names directly from Java backend\n",
    "# feature_names = summary._call_java(\"featureNames\")\n",
    "\n",
    "# 2. Construct full table including intercept\n",
    "features = [\"Intercept\"] + feature_names\n",
    "coefs = [lr_model.intercept] + list(lr_model.coefficients)\n",
    "se = list(summary.coefficientStandardErrors)\n",
    "tvals = list(summary.tValues)\n",
    "pvals = list(summary.pValues)\n",
    "\n",
    "print(\n",
    "\n",
    "print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n",
    "print(\"Length of features:\", len(features))\n",
    "print(\"Length of coefs:\", len(coefs))\n",
    "print(\"Length of se:\", len(se))\n",
    "print(\"Length of tvals:\", len(tvals))\n",
    "print(\"Length of pvals:\", len(pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Regression Summary\n",
    "The summary of the Generalized Linear Regression model provides important insights into the model's performance and the significance of each feature. The coefficients indicate the relationship between each feature and the target variable (salary), while the standard errors, t-values, and p-values help assess the reliability of these estimates.\n",
    "\n",
    "- Please interpret them in the context of your data and model. \n",
    "- Feature Names are purposefully not printed in the output. You can use the `features` variable to print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "#| echo: false\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "coef_table = pd.DataFrame({\n",
    "    # \"Feature\": features,\n",
    "    \"Estimate\": ,\n",
    "    \"Std Error\": ,\n",
    "    \"t-stat\": ,\n",
    "    \"P-Value\": \n",
    "})\n",
    "\n",
    "# 4. Optional pretty print\n",
    "print(tabulate(coef_table, headers=\"keys\", tablefmt=\"pretty\"))\n",
    "\n",
    "# 5. Save for report\n",
    "# coef_table.to_csv(\"_output/glr_summary_pretty.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Plot\n",
    "\n",
    "Diagnostic plots are essential for evaluating the performance of regression models. In this section, we will create several diagnostic plots to assess the linear regression model's assumptions and performance. There are four (2*2 grid) main plots we will create, you can use `seaborn` or `matplotlib` for this:\n",
    "\n",
    "1. **Predicted vs Actual Plot**\n",
    "2. **Residuals vs Predicted Plot**\n",
    "3. **Histogram of Residuals**\n",
    "4. **QQ Plot of Residuals**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "#| echo: false\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load predictions from GLR model\n",
    "df_pred = summary.predictions.select()\n",
    "\n",
    "# Compute residuals\n",
    "df_pred[\"residuals\"] = \n",
    "df_pred[\"fitted\"] = \n",
    "\n",
    "# Standardized residuals\n",
    "res_mean = \n",
    "res_std = \n",
    "df_pred[\"std_residuals\"] = \n",
    "\n",
    "# Square root of standardized residuals (for Scale-Location)\n",
    "df_pred[\"sqrt_std_resid\"] = \n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plot 1: Residuals vs Fitted\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "\n",
    "# Plot 2: Normal Q-Q\n",
    "plt.subplot(2, 2, 2)\n",
    "\n",
    "\n",
    "# Plot 3: Scale-Location\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "\n",
    "# Plot 4: Residuals vs Leverage — Approximate\n",
    "# Note: Leverage & Cook's Distance require X matrix; we approximate using fitted & residual\n",
    "plt.subplot(2, 2, 4)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"_output/glr_diagnostic_classic.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The evaluation of the model is crucial to understand its performance. In this section, we will calculate and visualize the following metrics:\n",
    "1. **R² (Coefficient of Determination)**: Indicates how well the model explains the variance in the target variable.\n",
    "2. **RMSE (Root Mean Squared Error)**: Measures the average magnitude of the errors between predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "#| echo: false\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, pow, sqrt, avg\n",
    "import numpy as np\n",
    "\n",
    "pred_glr = lr_model.transform(test_data)\n",
    "\n",
    "# R²\n",
    "r2_eval = \n",
    "r2 = \n",
    "# AIC from GLR summary\n",
    "aic = \n",
    "\n",
    "# BIC calculation\n",
    "n = \n",
    "k = \n",
    "rss = \n",
    "bic = \n",
    "\n",
    "# RMSE manually\n",
    "residuals_df = \n",
    "rmse = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Plot\n",
    "\n",
    "- Display the predicted vs actual salary plot with a red line indicating the ideal fit (y=x).\n",
    "- Use `seaborn` or `matplotlib` to create the plot.\n",
    "- Customize the plot with appropriate titles, labels, and legends.\n",
    "- Describe the plot in a few sentences, highlighting key insights and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "#| echo: false\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert GLR predictions to pandas\n",
    "pandas_df = \n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "plt.title(f\"Predicted vs Actual Salary (GLR Model)\\n\"\n",
    "          f\"RMSE = {rmse:.2f} | R² = {r2:.4f} | AIC = {aic:.2f} | BIC = {bic:.2f}\", loc=\"left\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\"_output/glr_predicted_vs_actual.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission {.unnumbered}\n",
    "1. Save figures in the `_output/` folder.\n",
    "2. Commit and push code and output files:\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add Lab 08 Salary Prediction models and output\"\n",
    "git push origin main\n",
    "```\n",
    "3. Submit your GitHub repository link.\n",
    "\n",
    "# Resources {.unnumbered}\n",
    "- [PySpark MLlib Docs](https://spark.apache.org/docs/latest/ml-guide.html)  \n",
    "- [Seaborn Docs](https://seaborn.pydata.org/)  \n",
    "- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
